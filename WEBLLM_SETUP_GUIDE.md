# WebLLM Browser-Based AI Setup Guide

## What is WebLLM?

WebLLM is a **completely browser-based AI** that:
- ‚úÖ Runs 100% on the user's device (no server needed)
- ‚úÖ Works completely offline after first model download
- ‚úÖ Free and open-source
- ‚úÖ Fast inference with WebGPU acceleration
- ‚úÖ No API keys required
- ‚úÖ Private - no data leaves the browser

## Browser Requirements

| Browser | Support | Min Version |
|---------|---------|-------------|
| Chrome  | ‚úÖ Full | 113+ |
| Edge    | ‚úÖ Full | 113+ |
| Opera   | ‚úÖ Full | 99+ |
| Firefox | ‚ö†Ô∏è Limited | Development |
| Safari  | ‚ùå No | - |

**Check support:** Open DevTools Console and run:
```javascript
console.log(await navigator.gpu?.requestAdapter() ? '‚úÖ WebGPU supported' : '‚ùå Not supported');
```

## How It Works

### Architecture
```
User Browser
‚îú‚îÄ‚îÄ React UI (QuestionBank.tsx)
‚îú‚îÄ‚îÄ localParser.ts (fast regex extraction)
‚îú‚îÄ‚îÄ WebLLM Runtime (Phi-2, Mistral, or TinyLlama)
‚îî‚îÄ‚îÄ Question Validation
```

### Flow
1. User uploads PDF/DOCX/TXT
2. Local parser extracts questions (fast, ~confidence%)
3. If confidence < 80%, WebLLM AI enhances extraction
4. WebLLM downloads model (~2GB, first time only)
5. Model runs locally in browser
6. Results displayed with confidence scores

## Setup Instructions

### For Users

**First Time Setup (one-time):**
1. Open your browser (Chrome/Edge/Opera 113+)
2. Navigate to the question bank
3. Upload a document (PDF/DOCX/TXT)
4. Click "Extract with AI"
5. Browser will download model (~2GB, takes 5-10 min on first run)
6. Extraction runs locally - your device will be busy for ~1-2 min
7. Results shown with confidence scores

**After First Setup:**
- Model cached in browser
- Instant extraction (no more downloads)
- Works offline!

### For Developers

**Installation:**
```bash
# Already included in dependencies
# WebLLM loads from CDN automatically
```

**Usage in Code:**
```typescript
import { extractQuestionsWithWebLLM, checkWebGPUSupport } from './lib/ai/webllmExtractor';

// Check support
const supported = await checkWebGPUSupport();
if (!supported) {
  console.log('WebGPU not supported, fall back to Groq/HuggingFace');
}

// Extract questions
const questions = await extractQuestionsWithWebLLM(text, 50, (msg) => {
  console.log(msg); // Progress updates
});
```

## Available Models

| Model | Size | Speed | Accuracy | Best For |
|-------|------|-------|----------|----------|
| TinyLlama | 1.1B | ‚ö°‚ö°‚ö° Fast | 60% | Low-spec devices |
| Phi-2 | 2.7B | ‚ö°‚ö° Fast | 75% | **Recommended** |
| Mistral | 7B | ‚ö° Medium | 85% | High-end devices |

Default: **Phi-2** (best balance of speed & accuracy)

## Troubleshooting

### "WebGPU not supported"
- **Issue:** Browser doesn't support WebGPU
- **Solution:** 
  - Update to Chrome/Edge/Opera 113+
  - Falls back to Groq API automatically

### "Model download failed"
- **Issue:** Network interrupted during 2GB download
- **Solution:**
  - Check internet connection
  - Try again (browser will resume)
  - Clear browser cache if stuck

### "Browser becomes unresponsive"
- **Issue:** AI inference is CPU-intensive
- **Solution:**
  - This is normal for first inference
  - Wait 1-2 minutes for completion
  - Close other tabs if needed
  - On low-end devices, use TinyLlama instead

### "Memory error"
- **Issue:** Device runs out of RAM
- **Solution:**
  - Use smaller model (TinyLlama)
  - Close other applications
  - Increase system RAM if possible

## Performance Specs

| Phase | Time | Device Impact |
|-------|------|----------------|
| Model download | 5-10 min | Network usage |
| Model load | 30-60 sec | CPU/GPU usage |
| First inference | 60-120 sec | CPU/GPU at 80%+ |
| Cached inference | 30-60 sec | CPU/GPU at 50%+ |

## Privacy & Security

### Data Safety
- ‚úÖ All processing happens in your browser
- ‚úÖ No data sent to servers
- ‚úÖ No tracking or analytics
- ‚úÖ Can work completely offline

### Model Source
- All models from HuggingFace
- Open-source and verified
- CDN cached for fast loading

## Fallback Hierarchy

If WebLLM unavailable:
1. **WebLLM** (browser-based) ‚Üê PRIMARY
2. **Ollama** (local server) if running
3. **Groq** (free cloud, 100k tokens/month)
4. **HuggingFace** (free cloud fallback)

## FAQ

**Q: Can I use this offline?**
A: Yes! After first model download, everything works offline.

**Q: Is my data private?**
A: 100% - all processing stays in your browser.

**Q: Can I share extracted questions?**
A: Yes, they're exported as JSON to Supabase (like before).

**Q: What if WebGPU isn't available?**
A: Falls back to Groq API automatically (still free).

**Q: Can I use a different model?**
A: Yes, modify model selection in `webllmExtractor.ts` line ~20.

**Q: Will this work on mobile?**
A: No, mobile browsers don't support WebGPU yet.

## Advanced Configuration

### Change Default Model
Edit `webllmExtractor.ts`:
```typescript
export async function initializeWebLLM(modelName: string = 'Mistral') {
  // Change 'Phi-2' to 'Mistral' or 'TinyLlama'
}
```

### Adjust Confidence Threshold
Edit `hybridExtractor.ts`:
```typescript
confidenceThreshold = 70; // Use AI for < 70% confidence (default 80)
```

### Custom Extraction Prompt
Edit `webllmExtractor.ts` line ~20:
```typescript
const EXTRACTION_PROMPT = `Your custom extraction instructions...`;
```

## Resources

- **WebLLM Official:** https://webllm.mlc.ai/
- **WebGPU Status:** https://gpuweb.github.io/gpuweb/
- **Browser Support:** https://caniuse.com/webgpu

## Next Steps

1. Update to Chrome/Edge 113+ if needed
2. Test on the question bank
3. Check browser console for any warnings
4. Report any issues!

---

**Summary:** WebLLM brings free, private, browser-based AI to the question extraction system. No servers, no API keys, no data tracking. Perfect for privacy-conscious educators! üéì‚ú®
